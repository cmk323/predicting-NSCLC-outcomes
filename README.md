# predicting-NSCLC-outcomes
Using simulated clinical and genomic data to build and evaluate a predictive model of one-year survival after diagnosis with NSCLC.

## Background
Lung cancer is the leading cause of cancer-related deaths worldwide. In 2018, the United States is estimated to have had approximately 234,030 new diagnoses of lung cancer, with an estimated 154,050 deaths. As lung cancer is a highly heterogeneous disease with variable survival outcomes, there has been wide interest in developing prognostic models that integrate varying sources of patient information (e.g. demographic, clinical, genomic, imaging) to predict patient survival following diagnosis. An eventual goal for these models is to deploy them as part of decision support systems to facilitate improved decision making in routine clinical practice.

This data, like our real data, may be messy, incomplete, and/or sparsely documented.
Please walk us through how you cleaned up this dataset. Do you see any interesting trends?

## Data Cleansing & Preprocessing
Several steps were taken to clean the data and preprocess it into a format which can be "understood" by a machine learning model. During an intitial exploration of the clinical data, several categorical columns stood out as those which may potentially contain inconsistencies in data entry, such as misspellings. For example, the **Stage** column contained one row in which the grade "IB" was entered using a different convention, "1B". Furthermore, two misspellings of "Right Upper Lobe" as "Righ Upper Lobe" in the **Primary.Site** column resulted in two separate categories being created for the same category. Fixing these errors helped improve data quality. Similarly, a discrepancy was identified in the **Grade** column, for which 96 rows contained values of "9". However, the data description notes that grade should range in value from 1-4. Because 96 rows occupies over 50% of the 190 row dataset, it was determined that this entire column should be dropped as the meaning of these "9" values cannot be deduced without consulting with individuals involved in data collection and entry.

Another important step in data cleansing was handling missing data. A substantial number of rows were identified with missing data for either the **N**, **M**, or **Tumor.Size** columns. Because the TNM System and tumor size intuitively seem like important prognostic factors, it would be illogical to drop these columns. Instead, all rows were preserved via mean imputation, in which `SimpleImputer()` was used to estimate the missing values using the mean. Mean imputation was used in this scenario due to time constraints. However, model accuracy could potentially be improved by using the TNM System to predict missing values of **T**, **N**, and **M**. Another potential strategy for estimating values of missing data in the **N**, **M**, or **Tumor.Size** columns is the implementation of `KNNImputer()`, which completes missing values using k-Nearest Neighbors.

In regards to preprocessing the data prior to building the model, one important step was to convert all categorical (string) data to numerical data, whether in the form of integers or Boolean flags. In order to accomplish this, the categories in **Stage** and **T** were simplified from a letter-number (e.x. "1a") ranking system to a solely numerical system (e.x. "1"). Additionally, one hot encoding was implemented to convert the categorical data in the **Histology** column as well as the at-risk genes of interest in the genomic data into new categorical columns with Boolean flags. For example, the three histological categories were divided into three new columns and appended to the dataframe, with a Boolean flag marking whether or not each histological feature was observed in a given patient. Similarly, three at-risk genes identified during exploratory data analysis were converted into separate categorical columns, with Boolean flags marking whether or not each patient was found via tumor sequencing to possess the gene variant of interest.
